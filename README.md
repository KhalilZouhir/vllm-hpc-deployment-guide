# üöÄ Gemma 27B (IT INT4 AWQ) Deployment with vLLM in HPC

This repository provides a complete guide for deploying and serving the **Gemma 27B INT4 AWQ** model on an HPC environment using **vLLM**, including PBS job setup, SSH tunneling, and local prompt testing.

---

## üß∞ Prerequisites

Before doing anything, make sure you have the following tools installed:

- **FortiClient VPN** ‚Üí *(link and details shared privately)*
- **MobaXterm** ‚Üí [Download here](https://mobaxterm.mobatek.net/download-home-edition.html)

These tools are required to connect to the HPC environment and manage sessions.

---

## 1Ô∏è‚É£ Setup Conda Environment

> **Note:** The environment and model directories already exist ‚Äî no need to recreate them.  
> Just activate and use the existing setup if available.

Create and activate the environment (if not already created):

```bash
conda create --name gptoss-vllm python=3.10
conda activate gptoss-vllm

## 2Ô∏è‚É£ Prepare the Model Directory

```bash
mkdir models
cd models

Go to your model directory (repository name) and copy it there.
<img width="763" height="439" alt="image" src="https://github.com/user-attachments/assets/1dcd2a64-d1a8-44b0-b370-81c54c273818" />


Download the model from Hugging Face:

```bash
huggingface-cli download gaunernst/gemma27b-it-int4-awq --local-dir ./gemma27b-it-int4-awq

## 3Ô∏è‚É£ Create the PBS Job Script

Prepare a directory to store your logs:

```bash
mkdir logs

Save the following as run_model_1.pbs:

```bash
#!/bin/bash
#PBS -N 1gpu
#PBS -l select=1:ncpus=20:mem=180gb:ngpus=1
#PBS -q gpu_1w
#PBS -o logs/output.log
#PBS -e logs/error.log

# === Load modules & activate conda ===
module use /app/common/modules
module load anaconda3-2024.10
source activate gptoss-vllm

# === Move to working directory ===
cd /home/skiredj.abderrahman/models

# === Confirm node ===
echo "==== Job running on node: $(hostname -s) ===="

# === Start vLLM server ===
vllm serve /home/skiredj.abderrahman/models/gemma27b-it-int4-awq \
  --tensor-parallel-size 1 \
  --port 9998


## 4Ô∏è‚É£ Submit the Job

Submit the PBS job:

```bash
qsub run_model_1.pbs

Check if it‚Äôs running:

```bash
qstat

## üìã Useful PBS Commands

* qstat -Qf ‚Üí Show all queue details

* qstat -f <job_id> ‚Üí Show detailed info for a specific job

* pbsnodes -a ‚Üí List all nodes and see where your model is running

## üîç Verifying the Deployment

Once the job is running, check which node it‚Äôs on:

```bash
qstat -f <job_id> | grep exec_host

Example:

```bash
qstat -f 5701.head1 | grep exec_host

You‚Äôll see something like exec_host = gpu08/..., meaning the model is running on gpu08.

## ‚úÖ Test the Model Inside the HPC Cluster

Execute this command on the login node (replace gpu08 with your actual node):

```bash
curl http://gpu08:9998/v1/completions -H "Content-Type: application/json" -d '{"model":"/home/skiredj.abderrahman/models/gemma27b-it-int4-awq","prompt":"hi how are u","max_tokens":50}'

## üåê Accessing the Model Locally (SSH Tunneling)

To access the API from your local machine, create an SSH tunnel to the GPU node:

```bash
ssh -N -L 127.0.0.1:9998:gpu08:9998 skiredj.abderrahman@172.30.30.11

Enter your HPC password when prompted.
Keep this terminal open while the tunnel is active.

## üí¨ Test a Prompt Locally

Open a new CMD window and run:

```bash
curl -X POST http://localhost:9998/v1/completions -H "Content-Type: application/json" -d "{\"model\":\"/home/skiredj.abderrahman/models/gemma27b-it-int4-awq\",\"prompt\":\"do u have a wish\",\"max_tokens\":50}"

If the setup is correct, you‚Äôll receive a JSON response containing the model‚Äôs output.

















